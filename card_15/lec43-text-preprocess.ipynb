{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer # classe que gera objetos para tokenização de texto\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # função que adiciona padding a sequências numéricas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T16:50:53.830873Z","iopub.execute_input":"2024-09-18T16:50:53.831628Z","iopub.status.idle":"2024-09-18T16:51:09.322063Z","shell.execute_reply.started":"2024-09-18T16:50:53.831580Z","shell.execute_reply":"2024-09-18T16:51:09.320841Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"sentences = [ # dataset fictício para demonstração de tokenização e padding\n    'I like eggs and ham.',\n    'I love chocolate and bunnies.',\n    'I hate onions'\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:51:42.268585Z","iopub.execute_input":"2024-09-18T16:51:42.269298Z","iopub.status.idle":"2024-09-18T16:51:42.275232Z","shell.execute_reply.started":"2024-09-18T16:51:42.269254Z","shell.execute_reply":"2024-09-18T16:51:42.273896Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"MAX_VOCAB_SIZE = 20000\n\"\"\"\nna língua inglesa, cerca de 3000 palavras correspondem a 95% de todo texto\ndisponível, então 20000 tokens deve ser mais do que suficiente para construir\num bom vocabulário de tokens.\n\"\"\"\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE) # cria um objeto para tokenização de um vocabulário com 20000 palavras\ntokenizer.fit_on_texts(sentences) # cria o vocabulário de tokens para o texto em sentences\nsequences = tokenizer.texts_to_sequences(sentences) # transforma as sentenças com o vocabulário criado","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:56:07.210794Z","iopub.execute_input":"2024-09-18T16:56:07.211232Z","iopub.status.idle":"2024-09-18T16:56:07.218357Z","shell.execute_reply.started":"2024-09-18T16:56:07.211192Z","shell.execute_reply":"2024-09-18T16:56:07.216583Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# visualizando as sentenças tokenizadas (sequências)\nsequences","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:56:44.087212Z","iopub.execute_input":"2024-09-18T16:56:44.087651Z","iopub.status.idle":"2024-09-18T16:56:44.097153Z","shell.execute_reply.started":"2024-09-18T16:56:44.087614Z","shell.execute_reply":"2024-09-18T16:56:44.095863Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[[1, 3, 4, 2, 5], [1, 6, 7, 2, 8], [1, 9, 10]]"},"metadata":{}}]},{"cell_type":"code","source":"# visualizando o mapeamento palavra-token\ntokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:57:42.511352Z","iopub.execute_input":"2024-09-18T16:57:42.512252Z","iopub.status.idle":"2024-09-18T16:57:42.520853Z","shell.execute_reply.started":"2024-09-18T16:57:42.512205Z","shell.execute_reply":"2024-09-18T16:57:42.519333Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'i': 1,\n 'and': 2,\n 'like': 3,\n 'eggs': 4,\n 'ham': 5,\n 'love': 6,\n 'chocolate': 7,\n 'bunnies': 8,\n 'hate': 9,\n 'onions': 10}"},"metadata":{}}]},{"cell_type":"code","source":"# aplicando o padding default da função pad_sequences\ndata = pad_sequences(sequences)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:58:35.362321Z","iopub.execute_input":"2024-09-18T16:58:35.362808Z","iopub.status.idle":"2024-09-18T16:58:35.372186Z","shell.execute_reply.started":"2024-09-18T16:58:35.362762Z","shell.execute_reply":"2024-09-18T16:58:35.370678Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[ 1,  3,  4,  2,  5],\n       [ 1,  6,  7,  2,  8],\n       [ 0,  0,  1,  9, 10]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"O default parece ser maxlen == len(maior vetor) e a localização do padding no início (pre).","metadata":{}},{"cell_type":"code","source":"# usando padding post\ndata = pad_sequences(sequences, padding = 'post')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-18T17:00:25.105443Z","iopub.execute_input":"2024-09-18T17:00:25.106361Z","iopub.status.idle":"2024-09-18T17:00:25.114876Z","shell.execute_reply.started":"2024-09-18T17:00:25.106317Z","shell.execute_reply":"2024-09-18T17:00:25.113569Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[ 1,  3,  4,  2,  5],\n       [ 1,  6,  7,  2,  8],\n       [ 1,  9, 10,  0,  0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"# testando padding exagerado\ndata = pad_sequences(sequences, maxlen = 6)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-18T17:01:01.042644Z","iopub.execute_input":"2024-09-18T17:01:01.043190Z","iopub.status.idle":"2024-09-18T17:01:01.053196Z","shell.execute_reply.started":"2024-09-18T17:01:01.043142Z","shell.execute_reply":"2024-09-18T17:01:01.051408Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([[ 0,  1,  3,  4,  2,  5],\n       [ 0,  1,  6,  7,  2,  8],\n       [ 0,  0,  0,  1,  9, 10]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"Observamos uma coluna inteira (a primeira) zerada. Isso é redundância, não ajuda em nada e só prejudica o aprendizado da rede.","metadata":{}},{"cell_type":"code","source":"# truncação\ndata = pad_sequences(sequences, maxlen = 4)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-18T17:02:01.147767Z","iopub.execute_input":"2024-09-18T17:02:01.148274Z","iopub.status.idle":"2024-09-18T17:02:01.157265Z","shell.execute_reply.started":"2024-09-18T17:02:01.148229Z","shell.execute_reply":"2024-09-18T17:02:01.155948Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[ 3,  4,  2,  5],\n       [ 6,  7,  2,  8],\n       [ 0,  1,  9, 10]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora, como maxlen foi assinalado para um valor menor do que a maior sequência, tivemos dois vetores truncas no início.<br>\nO truncamento no início, assim como o padding, faz mais sentido para RNNs, já que elas tendem a aprender mais com os inputs recentes a cada passo, \"esquecendo\" aqueles distantes no passado.","metadata":{}},{"cell_type":"code","source":"# também é possível truncar a parte final:\ndata = pad_sequences(sequences, maxlen = 4, padding = 'post')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-09-18T17:04:53.149558Z","iopub.execute_input":"2024-09-18T17:04:53.150616Z","iopub.status.idle":"2024-09-18T17:04:53.159836Z","shell.execute_reply.started":"2024-09-18T17:04:53.150564Z","shell.execute_reply":"2024-09-18T17:04:53.158350Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[ 3,  4,  2,  5],\n       [ 6,  7,  2,  8],\n       [ 1,  9, 10,  0]], dtype=int32)"},"metadata":{}}]}]}